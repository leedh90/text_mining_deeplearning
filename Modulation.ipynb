{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = None # 전역변수 선언"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import urllib.request\n",
    "from konlpy.tag import Okt\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# LSTM Training, Test\n",
    "from tensorflow.keras.layers import Embedding, Dense, LSTM\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 사전 제작\n",
    "stopwords = pd.read_csv(\"kostopword.txt\") # 한글 불용어 파일 불러오기\n",
    "stopwords = np.array(stopwords[\"stopword\"].tolist()) # 불용어 사전을 비교할 수 있게 리스트 형태로 형변환\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(data): # 입력값: 외부에서 가져온 데이터셋 / 출력값: Null 제거된 데이터셋\n",
    "                              # 기능: 중복 제거, 한글과 공백을 제외하고 모두 제거, Null값 제거\n",
    "    \n",
    "    # 중복 여부 검사, nunique()는 중복값을 제외한 유니크한 값의 갯수를 카운팅 해줌.\n",
    "    data['document'].nunique(), data['label'].nunique()\n",
    "    \n",
    "    # document 컬럼에서 중복인 내용이 있으면 중복 제거\n",
    "    data.drop_duplicates(subset=['document'], inplace=True)\n",
    "    # Null값이 존재하는 행 제거\n",
    "    data = data.dropna(axis=0)\n",
    "    \n",
    "    # 한글과 공백을 제외하고 모두 제거하는 정규 표현식 사용\n",
    "    data['document'] = data['document'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\", \"\")\n",
    "    \n",
    "    # 다시한번 Null값이 존재하는지 확인\n",
    "    data['document'] = data['document'].str.replace('^ +', \"\") # 공백만 있거나 빈 값을 가진 행이 있으면 빈문자열로 변경\n",
    "    data['document'].replace('', np.nan, inplace=True) # 빈문자열을 Null값으로 변환\n",
    "    \n",
    "    # Train 데이터의 null 행 제거\n",
    "    data = data.dropna(axis=0)\n",
    "    \n",
    "    # 최종적으로 한글을 제외한 데이터 및 모든 결측치를 제거한 dataset이 리턴됨.\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(data):  # 입력값: Null값이 제거된 데이터셋 / 출력값: 불용어를 제거한 후 Null값도 제거한 데이터셋\n",
    "                            # 기능: 불용어 제거, 단어 빈도수가 2회 이하인 단어 수를 찾아내고 공백 제거(공백 제거 목적)\n",
    "    # 토큰화한 값을 X_data의 리스트로 넣어줌.\n",
    "    X_data=[]\n",
    "    # 데이터 처리가 완료된 데이터셋의 document 컬럼값들만 토큰화 진행\n",
    "    for sentence in data['document']:\n",
    "        # 형태소 분석기(Okt())에서 토큰화(한글은 띄어쓰기) 실행. stem=True로 일정 수준 정규화(동사,명사화)\n",
    "        X_tmp = okt.morphs(sentence, stem=True)\n",
    "        X_tmp = [word for word in X_tmp if not word in stopwords] # 불용어 사전에 없으면 리스트에 추가\n",
    "        X_data.append(X_tmp)\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(X_data)\n",
    "    \n",
    "    # 등장 빈도수가 3회 미만인 단어들이 이 데이터에서 얼만큼 비중을 차지하는지 확인\n",
    "    bindo = 3 # 단어의 등장 빈도수 기준\n",
    "    rare_cnt = 0 # 등장 빈도수가 bindo보다 작은 단어의 개수를 카운트\n",
    "    total_freq = 0 # 훈련 데이터의 전체 단어 빈도수 총 합\n",
    "    rare_freq = 0 # 등장 빈도수가 bindo보다 작은 단어의 빈도수 총 합\n",
    "    total_cnt = len(tokenizer.word_index) # 단어의 수\n",
    "    \n",
    "    # key-value 형태로 저장\n",
    "    for key, value in tokenizer.word_counts.items():\n",
    "        total_freq = total_freq + value\n",
    "\n",
    "        # 단어의 등장 빈도수가 bindo보다 작으면 아래 if 문을 실행\n",
    "        if(value < bindo):\n",
    "            rare_cnt = rare_cnt + 1\n",
    "            rare_freq = rare_freq + value\n",
    "\n",
    "    # 단어 빈도수가 2회 이하인 단어들은 제외\n",
    "    # 0번을 고려해서 크기는 +1을 해준다.\n",
    "    voca_size = total_cnt - rare_cnt + 1\n",
    "    \n",
    "    tokenizer = Tokenizer(voca_size)\n",
    "    tokenizer.fit_on_texts(X_data)\n",
    "    \n",
    "    y_data = np.array(data['label'])\n",
    "    global tk\n",
    "    tk = tokenizer\n",
    "    X_data, y_data = train_padding(X_data, y_data)\n",
    "    \n",
    "    return X_data, y_data, voca_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_padding(X_data, y_data):\n",
    "    \n",
    "    X_data = tk.texts_to_sequences(X_data) # texts_to_sequences > 단어들에 순번을 지정\n",
    "\n",
    "    # empty samples 제거\n",
    "    drop_data = [index for index, sentence in enumerate(X_data) if len(sentence) < 1] # 단어가 1개 미만, 즉 비어있는 데이터 제거\n",
    "\n",
    "    # 빈 샘플 제거\n",
    "    X_data = np.delete(X_data, drop_data, axis=0) # X_data에서 drop_data을 사용해서 제거\n",
    "    y_data = np.delete(y_data, drop_data, axis=0)\n",
    "    \n",
    "    # 전체 훈련 데이터중 대부분의 데이터 길이가 35 이하이므로 모든 샘플의 길이를 35으로 조정\n",
    "    X_data = pad_sequences(X_data, maxlen = 35)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_tokenizer(data):  # 입력값: Null값이 제거된 데이터셋 / 출력값: 불용어를 제거한 후 Null값도 제거한 데이터셋\n",
    "                            # 기능: 불용어 제거, 단어 빈도수가 2회 이하인 단어 수를 찾아내고 공백 제거(공백 제거 목적)\n",
    "    X_data=[]\n",
    "    for sentence in data['document']:\n",
    "        # 형태소 분석기(Okt())에서 토큰화(한글은 띄어쓰기) 실행. stem=True로 일정 수준 정규화(동사,명사화)\n",
    "        X_tmp = okt.morphs(sentence, stem=True)\n",
    "        X_tmp = [word for word in X_tmp if not word in stopwords] # 불용어 사전에 없으면 리스트에 추가\n",
    "        X_data.append(X_tmp)\n",
    "\n",
    "    y_data = np.array(data['label'])\n",
    "\n",
    "    X_data = test_padding(X_data)\n",
    "    \n",
    "    return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_padding(X_data):\n",
    "\n",
    "    X_data = tk.texts_to_sequences(X_data) # texts_to_sequences > 단어들에 순번을 지정\n",
    "\n",
    "    # 전체 훈련 데이터중 대부분의 데이터 길이가 35 이하이므로 모든 샘플의 길이를 35으로 조정\n",
    "    X_data = pad_sequences(X_data, maxlen = 35)\n",
    "\n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainAndTest(X_data, y_data, X_data2, y_data2, voca_size): # 입력값: / 출력값: /함수의 기능:\n",
    "\n",
    "    model = Sequential()\n",
    "    # 케라스 시퀀셜을 하나의 입력과 출력을 가짐 다음 셀로 이동시키는 기능!\n",
    "    model.add(Embedding(voca_size, 100)) # 정수화 시키는 것(전체 단어의 집합을 벡터의 크기를 100으로 임베딩한다.)\n",
    "    model.add(LSTM(128)) # 128개의 셀로 LSTM을 사용\n",
    "    model.add(Dense(1, activation='sigmoid')) # 처음 인자 출력 갯수, 활성 함수는 sigmoid사용(binary classification)\n",
    "\n",
    "    # 검증 데이터 손실이 증가하면, 과적합 위험. 검증 데이터 손실이 4회 증가하면 학습을 조기 종료\n",
    "    # ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다 좋아질 경우에만 모델 저장\n",
    "    \n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n",
    "    # 조건에 해당하면 처리 과정을 멈춰라, val_loss를 모니터링 하고 손실값이기 때문에 mode는 최소인 min으로 설정\n",
    "    # verbose=1은 언제 멈췄는지 화면에 찍어주는 역할, patience 성능이 증가하지 않을때 몇번 더 시도할 것인지 지정\n",
    "    mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\n",
    "    # 정확도를 모니터하여 최대값을 가지는 파라미터 모델을 best_model.h5 이름으로 저장한다.\n",
    "\n",
    "    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n",
    "    # 모델을 컴파일 할때는 정규화기, 손실함수, 평가지표 3가지를 설정한다.\n",
    "    # rmsprop는 최신 기울기의 반영 비율이 더 높은 방식, binary_corssentropy는 output layer가 sigmod일떄 사용\n",
    "    # 분류값이기 때문에 평가지표는 정확도를 사용한다.\n",
    "    history = model.fit(X_train, y_train, epochs=15, callbacks=[es, mc], batch_size=60, validation_split=0.2)\n",
    "    # epcohs는 15회를 사용하고 20%의 데이터를 검증용으로 분류한다.\n",
    "    # 얼리스탑과 모델체크포인트를 콜백하여 사용한다.\n",
    "\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    # 모델체크포인트로 성능이 가장 좋은 모델을 가져온다.\n",
    "    # 성능이 가장 좋은 모델로 테스트용 데이터를 검증한다.\n",
    "    print(\"\\n 테스트 정확도 : %0.4f\" % (loaded_model.evaluate(X_data2, y_data2)[1]))\n",
    "    # 0번 인덱스는 loss, 1번 인덱스는 정확도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentimemt_predict(new_sentence):\n",
    "#     a, b, c, tokenizer = data_prediction(np.array(new_sentence))\n",
    "    loaded_model = load_model('best_model.h5')\n",
    "    new_sentence = okt.morphs(new_sentence, stem=True) #토큰화\n",
    "    new_sentence = [word for word in new_sentence if not word in stopwords] #불용어 \n",
    "    \n",
    "    pad_new = test_padding([new_sentence])\n",
    "#     encoded = tk.texts_to_sequences([new_sentence]) #정수 인코딩\n",
    "#     pad_new = pad_sequences(encoded, maxlen = 35) #패딩\n",
    "    score = float(loaded_model.predict(pad_new)) #예측\n",
    "    if(score > 0.5):\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_table('ratings_train.txt')\n",
    "test_data = pd.read_table('ratings_test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data_preprocessing(train_data)\n",
    "test_data = data_preprocessing(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, voca_size = train_tokenizer(train_data)\n",
    "X_test, y_test = test_tokenizer(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1933/1934 [============================>.] - ETA: 0s - loss: 0.3922 - acc: 0.8214\n",
      "Epoch 00001: val_acc improved from -inf to 0.83642, saving model to best_model.h5\n",
      "1934/1934 [==============================] - 72s 37ms/step - loss: 0.3922 - acc: 0.8214 - val_loss: 0.3654 - val_acc: 0.8364\n",
      "Epoch 2/15\n",
      "1933/1934 [============================>.] - ETA: 0s - loss: 0.3293 - acc: 0.8559\n",
      "Epoch 00002: val_acc improved from 0.83642 to 0.85183, saving model to best_model.h5\n",
      "1934/1934 [==============================] - 72s 37ms/step - loss: 0.3293 - acc: 0.8559 - val_loss: 0.3374 - val_acc: 0.8518\n",
      "Epoch 3/15\n",
      "1933/1934 [============================>.] - ETA: 0s - loss: 0.3036 - acc: 0.8705\n",
      "Epoch 00003: val_acc improved from 0.85183 to 0.85690, saving model to best_model.h5\n",
      "1934/1934 [==============================] - 73s 38ms/step - loss: 0.3036 - acc: 0.8705 - val_loss: 0.3303 - val_acc: 0.8569\n",
      "Epoch 4/15\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.2851 - acc: 0.8810\n",
      "Epoch 00004: val_acc improved from 0.85690 to 0.85790, saving model to best_model.h5\n",
      "1934/1934 [==============================] - 76s 39ms/step - loss: 0.2851 - acc: 0.8810 - val_loss: 0.3297 - val_acc: 0.8579\n",
      "Epoch 5/15\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.2693 - acc: 0.8888\n",
      "Epoch 00005: val_acc did not improve from 0.85790\n",
      "1934/1934 [==============================] - 79s 41ms/step - loss: 0.2693 - acc: 0.8888 - val_loss: 0.3327 - val_acc: 0.8552\n",
      "Epoch 6/15\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.2548 - acc: 0.8961\n",
      "Epoch 00006: val_acc did not improve from 0.85790\n",
      "1934/1934 [==============================] - 76s 39ms/step - loss: 0.2548 - acc: 0.8961 - val_loss: 0.3344 - val_acc: 0.8554\n",
      "Epoch 7/15\n",
      "1934/1934 [==============================] - ETA: 0s - loss: 0.2400 - acc: 0.9034\n",
      "Epoch 00007: val_acc did not improve from 0.85790\n",
      "1934/1934 [==============================] - 74s 38ms/step - loss: 0.2400 - acc: 0.9034 - val_loss: 0.3519 - val_acc: 0.8498\n",
      "Epoch 8/15\n",
      "1933/1934 [============================>.] - ETA: 0s - loss: 0.2246 - acc: 0.9107\n",
      "Epoch 00008: val_acc did not improve from 0.85790\n",
      "1934/1934 [==============================] - 78s 40ms/step - loss: 0.2246 - acc: 0.9107 - val_loss: 0.3531 - val_acc: 0.8502\n",
      "Epoch 00008: early stopping\n",
      "1527/1527 [==============================] - 9s 6ms/step - loss: 0.3405 - acc: 0.8530\n",
      "\n",
      " 테스트 정확도 : 0.8530\n"
     ]
    }
   ],
   "source": [
    "# Training(X_train, y_train, voca_size)\n",
    "TrainAndTest(X_train, y_train, X_test, y_test, voca_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.77% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('재밌어요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63.19% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('한번쯤은 볼만한 영화')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.68% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('쓰레기')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65.84% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('보라고 하고싶진 않지만 한번은 보세요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018E7C3E2E18> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "82.11% 확률로 긍정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('꼭 보고싶다면 말리진 않을게요')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018E7D565BF8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "96.93% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('이걸 돈주고 본다고?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:7 out of the last 7 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018E7CBA4AE8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "61.64% 확률로 부정 리뷰입니다.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentimemt_predict('개행개행')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
